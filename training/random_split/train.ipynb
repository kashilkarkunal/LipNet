{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, CSVLogger, ModelCheckpoint\n",
    "from lipnet.lipreading.generators import RandomSplitGenerator\n",
    "from lipnet.lipreading.callbacks import Statistics, Visualize\n",
    "from lipnet.lipreading.curriculums import Curriculum\n",
    "from lipnet.core.decoders import Decoder\n",
    "from lipnet.lipreading.helpers import labels_to_text\n",
    "from lipnet.utils.spell import Spell\n",
    "from lipnet.new_model import LipNet\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/home/home-02/56/656/kkashilk/LipNet/training/random_split/datasets\n",
      "\n",
      "Loading dataset list from cache...\n",
      "Found 739 videos for training.\n",
      "Found 184 videos for validation.\n",
      "\n",
      "WARNING:tensorflow:From /datasets/home/56/656/kkashilk/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /datasets/home/56/656/kkashilk/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1044: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /datasets/home/56/656/kkashilk/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1008: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, 75, 100, 50, 3)    0         \n",
      "_________________________________________________________________\n",
      "zero1 (ZeroPadding3D)        (None, 77, 104, 54, 3)    0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv3D)               (None, 75, 50, 25, 32)    7232      \n",
      "_________________________________________________________________\n",
      "batc1 (BatchNormalization)   (None, 75, 50, 25, 32)    128       \n",
      "_________________________________________________________________\n",
      "actv1 (Activation)           (None, 75, 50, 25, 32)    0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout3d_1 (Spatial (None, 75, 50, 25, 32)    0         \n",
      "_________________________________________________________________\n",
      "max1 (MaxPooling3D)          (None, 75, 25, 12, 32)    0         \n",
      "_________________________________________________________________\n",
      "zero2 (ZeroPadding3D)        (None, 77, 29, 16, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 75, 25, 12, 64)    153664    \n",
      "_________________________________________________________________\n",
      "batc2 (BatchNormalization)   (None, 75, 25, 12, 64)    256       \n",
      "_________________________________________________________________\n",
      "actv2 (Activation)           (None, 75, 25, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout3d_2 (Spatial (None, 75, 25, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "max2 (MaxPooling3D)          (None, 75, 12, 6, 64)     0         \n",
      "_________________________________________________________________\n",
      "zero3 (ZeroPadding3D)        (None, 77, 14, 8, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv3D)               (None, 75, 12, 6, 96)     165984    \n",
      "_________________________________________________________________\n",
      "batc3 (BatchNormalization)   (None, 75, 12, 6, 96)     384       \n",
      "_________________________________________________________________\n",
      "actv3 (Activation)           (None, 75, 12, 6, 96)     0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout3d_3 (Spatial (None, 75, 12, 6, 96)     0         \n",
      "_________________________________________________________________\n",
      "max3 (MaxPooling3D)          (None, 75, 6, 3, 96)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 75, 1728)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 75, 512)           4065280   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 75, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 75, 28)            14364     \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 75, 28)            0         \n",
      "=================================================================\n",
      "Total params: 5,982,204.0\n",
      "Trainable params: 5,981,820.0\n",
      "Non-trainable params: 384.0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /datasets/home/56/656/kkashilk/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 0: Curriculum(train: True, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "Epoch 1/100\n",
      "29/29 [============================>.] - ETA: 2s - loss: 94.9281Epoch 0: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "Epoch 0: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/56/656/kkashilk/.local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/datasets/home/56/656/kkashilk/.local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/datasets/home/56/656/kkashilk/.local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Epoch 0] Out of 256 samples: [CER: 20.523 - 0.824] [WER: 5.973 - 0.995] [BLEU: 0.389 - 0.389]\n",
      "\n",
      "30/29 [==============================] - 240s - loss: 94.2674 - val_loss: 72.0675\n",
      "Epoch 2/100\n",
      "29/29 [============================>.] - ETA: 1s - loss: 70.8645Epoch 1: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "Epoch 1: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "\n",
      "\n",
      "[Epoch 1] Out of 256 samples: [CER: 24.941 - 1.000] [WER: 6.000 - 1.000] [BLEU: 0.000 - 0.000]\n",
      "\n",
      "30/29 [==============================] - 171s - loss: 70.9529 - val_loss: 70.6434\n",
      "Epoch 3/100\n",
      "29/29 [============================>.] - ETA: 1s - loss: 69.6207Epoch 2: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "Epoch 2: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "\n",
      "\n",
      "[Epoch 2] Out of 256 samples: [CER: 24.926 - 1.000] [WER: 6.000 - 1.000] [BLEU: 0.000 - 0.000]\n",
      "\n",
      "30/29 [==============================] - 174s - loss: 69.7135 - val_loss: 69.4462\n",
      "Epoch 4/100\n",
      "29/29 [============================>.] - ETA: 1s - loss: 68.6865Epoch 3: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "Epoch 3: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "\n",
      "\n",
      "[Epoch 3] Out of 256 samples: [CER: 24.125 - 0.967] [WER: 5.977 - 0.996] [BLEU: 0.373 - 0.373]\n",
      "\n",
      "30/29 [==============================] - 173s - loss: 68.7732 - val_loss: 68.4192\n",
      "Epoch 5/100\n",
      "29/29 [============================>.] - ETA: 1s - loss: 67.6504Epoch 4: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "Epoch 4: Curriculum(train: False, sentence_length: -1, flip_probability: 0.5, jitter_probability: 0.05)\n",
      "\n",
      "\n",
      "[Epoch 4] Out of 256 samples: [CER: 23.891 - 0.963] [WER: 5.961 - 0.993] [BLEU: 0.409 - 0.409]\n",
      "\n",
      "30/29 [==============================] - 159s - loss: 67.7354 - val_loss: 67.4911\n",
      "Epoch 6/100\n",
      "20/29 [===================>..........] - ETA: 26s - loss: 66.8149"
     ]
    }
   ],
   "source": [
    "np.random.seed(55)\n",
    "\n",
    "CURRENT_PATH = os.path.abspath('')\n",
    "DATASET_DIR  = os.path.join(CURRENT_PATH, 'datasets')\n",
    "print(DATASET_DIR)\n",
    "OUTPUT_DIR   = os.path.join(CURRENT_PATH, 'results')\n",
    "LOG_DIR      = os.path.join(CURRENT_PATH, 'logs')\n",
    "FACE_PREDICTORS = os.path.join(CURRENT_PATH, '..', '..', 'common', 'predictors', 'shape_predictor_68_face_landmarks.dat')\n",
    "PREDICT_GREEDY      = False\n",
    "PREDICT_BEAM_WIDTH  = 200\n",
    "PREDICT_DICTIONARY  = os.path.join(CURRENT_PATH,'..','..','common','dictionaries','grid.txt')\n",
    "\n",
    "def curriculum_rules(epoch):\n",
    "    return { 'sentence_length': -1, 'flip_probability': 0.5, 'jitter_probability': 0.05 }\n",
    "\n",
    "def train(run_name, start_epoch, stop_epoch, img_c, img_w, img_h, frames_n, absolute_max_string_len, minibatch_size):\n",
    "    curriculum = Curriculum(curriculum_rules)\n",
    "    lip_gen = RandomSplitGenerator(dataset_path=DATASET_DIR,\n",
    "                                minibatch_size=minibatch_size,\n",
    "                                img_c=img_c, img_w=img_w, img_h=img_h, frames_n=frames_n,\n",
    "                                absolute_max_string_len=absolute_max_string_len,\n",
    "                                curriculum=curriculum, start_epoch=start_epoch).build(val_split=0.2)\n",
    "\n",
    "    lipnet = LipNet(img_c=img_c, img_w=img_w, img_h=img_h, frames_n=frames_n, \n",
    "                            absolute_max_string_len=absolute_max_string_len, output_size=lip_gen.get_output_size())\n",
    "    lipnet.summary()\n",
    "\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "    lipnet.model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)\n",
    "\n",
    "    # load weight if necessary\n",
    "    if start_epoch > 0:\n",
    "        weight_file = os.path.join(OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1)))\n",
    "        lipnet.model.load_weights(weight_file)\n",
    "\n",
    "    spell = Spell(path=PREDICT_DICTIONARY)\n",
    "    decoder = Decoder(greedy=PREDICT_GREEDY, beam_width=PREDICT_BEAM_WIDTH,\n",
    "                      postprocessors=[labels_to_text, spell.sentence])\n",
    "\n",
    "    # define callbacks\n",
    "    statistics  = Statistics(lipnet, lip_gen.next_val(), decoder, 256, output_dir=os.path.join(OUTPUT_DIR, run_name))\n",
    "    visualize   = Visualize(os.path.join(OUTPUT_DIR, run_name), lipnet, lip_gen.next_val(), decoder, num_display_sentences=minibatch_size)\n",
    "    tensorboard = TensorBoard(log_dir=os.path.join(LOG_DIR, run_name))\n",
    "    csv_logger  = CSVLogger(os.path.join(LOG_DIR, \"{}-{}.csv\".format('training',run_name)), separator=',', append=True)\n",
    "    checkpoint  = ModelCheckpoint(os.path.join(OUTPUT_DIR, run_name, \"weights{epoch:02d}.h5\"), monitor='val_loss', save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "    lipnet.model.fit_generator(generator=lip_gen.next_train(), \n",
    "                        steps_per_epoch=lip_gen.default_training_steps, epochs=stop_epoch,\n",
    "                        validation_data=lip_gen.next_val(), validation_steps=lip_gen.default_validation_steps,\n",
    "                        callbacks=[checkpoint, statistics, visualize, lip_gen, tensorboard, csv_logger], \n",
    "                        initial_epoch=start_epoch, \n",
    "                        verbose=1,\n",
    "                        max_q_size=5,\n",
    "                        workers=1,\n",
    "                        pickle_safe=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_name = \"attention_layer\"\n",
    "    train(run_name, 0, 100, 3, 100, 50, 75, 32, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
